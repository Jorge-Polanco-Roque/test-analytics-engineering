name: Data Quality Monitor

# ============================================================================
# GITHUB ACTIONS - DATA QUALITY MONITORING
# ============================================================================
# Pipeline dedicado a monitoreo continuo de calidad de datos
#
# Funcionalidades:
#   - Monitoreo programado de calidad de datos
#   - Alertas autom√°ticas cuando fallan tests
#   - Reportes de m√©tricas de calidad
#   - Tracking de tendencias de calidad
#
# Autor: Analytics Engineering Team
# ============================================================================

on:
  # Ejecuci√≥n programada (diaria a las 7 AM UTC)
  schedule:
    - cron: '0 7 * * *'

  # Permitir ejecuci√≥n manual
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'
  DBT_VERSION: '1.7.9'

jobs:
  # ==========================================================================
  # JOB 1: MONITOREO DE FRESHNESS
  # ==========================================================================
  check-freshness:
    name: Check Data Freshness
    runs-on: ubuntu-latest

    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4

      - name: üêç Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: üì¶ Install DBT
        run: |
          pip install dbt-core==${{ env.DBT_VERSION }}
          pip install dbt-bigquery==${{ env.DBT_VERSION }}

      - name: üîê Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}

      - name: üîß Create DBT profiles.yml
        run: |
          mkdir -p ~/.dbt
          cat > ~/.dbt/profiles.yml << EOF
          bank_marketing:
            target: prod
            outputs:
              dev:
                type: bigquery
                method: service-account
                project: ${{ secrets.DBT_GCP_PROJECT }}
                dataset: bank_marketing_dev
                location: US
                threads: 4
                timeout_seconds: 300
                priority: interactive
                keyfile: /tmp/gcp-key.json
              staging:
                type: bigquery
                method: service-account
                project: ${{ secrets.DBT_GCP_PROJECT_STAGING }}
                dataset: bank_marketing_staging
                location: US
                threads: 2
                timeout_seconds: 300
                priority: interactive
                keyfile: /tmp/gcp-key.json
              prod:
                type: bigquery
                method: service-account
                project: ${{ secrets.DBT_GCP_PROJECT_PROD }}
                dataset: bank_marketing_prod
                location: US
                threads: 1
                timeout_seconds: 300
                priority: interactive
                keyfile: /tmp/gcp-key.json
          EOF

      - name: üîê Create GCP credentials file
        run: |
          echo '${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}' > /tmp/gcp-key.json

      - name: üìö Install DBT packages
        run: |
          cd dbt_project
          dbt deps || true

      - name: üîç Check source freshness
        id: freshness
        run: |
          cd dbt_project
          dbt source freshness --target prod
        env:
          DBT_GCP_PROJECT: ${{ secrets.DBT_GCP_PROJECT_PROD }}
        continue-on-error: true

      - name: üö® Create issue on freshness failure
        if: steps.freshness.outcome == 'failure'
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'üö® Data Freshness Alert: Stale Data Detected',
              body: `## Data Freshness Check Failed

              The scheduled freshness check has detected stale data in our sources.

              **Details:**
              - Check Time: ${new Date().toISOString()}
              - Workflow Run: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}

              **Action Required:**
              1. Investigate data pipeline delays
              2. Check source data availability
              3. Contact data engineering team if issue persists

              **Auto-generated by:** Data Quality Monitor
              `,
              labels: ['data-quality', 'alert', 'freshness']
            })

  # ==========================================================================
  # JOB 2: TESTS DE CALIDAD
  # ==========================================================================
  quality-tests:
    name: Run Quality Tests
    runs-on: ubuntu-latest

    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4

      - name: üêç Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: üì¶ Install DBT
        run: |
          pip install dbt-core==${{ env.DBT_VERSION }}
          pip install dbt-bigquery==${{ env.DBT_VERSION }}

      - name: üîê Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}

      - name: üîß Create DBT profiles.yml
        run: |
          mkdir -p ~/.dbt
          cat > ~/.dbt/profiles.yml << EOF
          bank_marketing:
            target: prod
            outputs:
              dev:
                type: bigquery
                method: service-account
                project: ${{ secrets.DBT_GCP_PROJECT }}
                dataset: bank_marketing_dev
                location: US
                threads: 4
                timeout_seconds: 300
                priority: interactive
                keyfile: /tmp/gcp-key.json
              staging:
                type: bigquery
                method: service-account
                project: ${{ secrets.DBT_GCP_PROJECT_STAGING }}
                dataset: bank_marketing_staging
                location: US
                threads: 2
                timeout_seconds: 300
                priority: interactive
                keyfile: /tmp/gcp-key.json
              prod:
                type: bigquery
                method: service-account
                project: ${{ secrets.DBT_GCP_PROJECT_PROD }}
                dataset: bank_marketing_prod
                location: US
                threads: 1
                timeout_seconds: 300
                priority: interactive
                keyfile: /tmp/gcp-key.json
          EOF

      - name: üîê Create GCP credentials file
        run: |
          echo '${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}' > /tmp/gcp-key.json

      - name: üìö Install DBT packages
        run: |
          cd dbt_project
          dbt deps || true

      - name: üß™ Run all tests
        id: tests
        run: |
          cd dbt_project
          dbt test --target prod --store-failures
        env:
          DBT_GCP_PROJECT: ${{ secrets.DBT_GCP_PROJECT_PROD }}
        continue-on-error: true

      - name: üìä Generate quality report
        run: |
          echo "## üìä Data Quality Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Report Date:** $(date -u)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ steps.tests.outcome }}" == "success" ]; then
            echo "‚úÖ **Status:** All tests passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå **Status:** Some tests failed" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Failed records have been stored in BigQuery test_results schema." >> $GITHUB_STEP_SUMMARY
          fi

      - name: üö® Create issue on test failure
        if: steps.tests.outcome == 'failure'
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'üö® Data Quality Alert: Tests Failed',
              body: `## Data Quality Tests Failed

              Automated data quality tests have detected issues in production data.

              **Details:**
              - Check Time: ${new Date().toISOString()}
              - Workflow Run: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}

              **Failed Records:**
              Failed test records are stored in BigQuery:
              - Project: Check workflow environment variables
              - Dataset: \`test_results\`

              **Action Required:**
              1. Review failed records in BigQuery
              2. Investigate root cause
              3. Implement data corrections if needed
              4. Update tests if business rules changed

              **Auto-generated by:** Data Quality Monitor
              `,
              labels: ['data-quality', 'alert', 'test-failure']
            })

  # ==========================================================================
  # JOB 3: M√âTRICAS DE NEGOCIO
  # ==========================================================================
  business-metrics:
    name: Monitor Business Metrics
    runs-on: ubuntu-latest

    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4

      - name: üêç Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: üì¶ Install dependencies
        run: |
          pip install google-cloud-bigquery
          pip install pandas

      - name: üîê Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}

      - name: üìà Check key metrics
        run: |
          python3 << 'EOF'
          from google.cloud import bigquery
          import os

          client = bigquery.Client(project=os.environ['DBT_GCP_PROJECT_PROD'])

          # Query para obtener m√©tricas clave
          query = """
          SELECT
              AVG(conversion_rate_pct) as avg_conversion_rate,
              COUNT(*) as total_segments,
              SUM(total_contacts) as total_contacts,
              SUM(successful_contacts) as total_conversions
          FROM `{project}.marts.kpi_bank_marketing`
          """.format(project=os.environ['DBT_GCP_PROJECT_PROD'])

          results = client.query(query).result()

          for row in results:
              print(f"Average Conversion Rate: {row.avg_conversion_rate:.2f}%")
              print(f"Total Segments: {row.total_segments}")
              print(f"Total Contacts: {row.total_contacts}")
              print(f"Total Conversions: {row.total_conversions}")

              # Alerta si conversion rate < 10%
              if row.avg_conversion_rate < 10.0:
                  print("‚ö†Ô∏è WARNING: Average conversion rate below target!")
                  exit(1)
          EOF
        env:
          DBT_GCP_PROJECT_PROD: ${{ secrets.DBT_GCP_PROJECT_PROD }}
        continue-on-error: true

      - name: üìä Generate metrics summary
        run: |
          echo "## üìà Business Metrics Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Daily metrics monitoring completed." >> $GITHUB_STEP_SUMMARY
          echo "Check workflow logs for detailed metrics." >> $GITHUB_STEP_SUMMARY

# ============================================================================
# CONFIGURACI√ìN DE ALERTAS
# ============================================================================
#
# Este workflow crea autom√°ticamente GitHub Issues cuando:
# 1. Los datos est√°n desactualizados (freshness check fails)
# 2. Los tests de calidad fallan
# 3. Las m√©tricas de negocio est√°n fuera de rango
#
# Para habilitar notificaciones adicionales (Slack, Email, PagerDuty):
# 1. Agregar secrets correspondientes en GitHub
# 2. Descomentar y configurar las secciones de notificaci√≥n
#
# ============================================================================
